{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83045409-70a5-4010-b1cf-9db8d98f2bad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "059a269d-8f13-494e-84e5-9da5d873047b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Joachim Maksim, joachmak\n",
    "\n",
    "JÃ¸rgen Nordli Katralen, jorgennk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52939e3d-35c7-4767-a59e-ba75f716e952",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\r\n",
       "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-fb09ed1e-5b42-4f34-a4e5-9304e1deb16c/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-fb09ed1e-5b42-4f34-a4e5-9304e1deb16c/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n",
    "!pip install -q ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "499e46f5-12a4-46de-a212-90f2e936461e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading modules that we need\n",
    "import unittest\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f8c68e0-6bda-40a6-8588-381e51dc0a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \n",
    "def load_df(table_name: \"name of the table to load\") -> DataFrame:\n",
    "    return spark.read.parquet(table_name)\n",
    "\n",
    "users_df = load_df(\"/user/hive/warehouse/users\")\n",
    "comments_df = load_df(\"/user/hive/warehouse/comments\")\n",
    "posts_df = load_df(\"/user/hive/warehouse/posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2274f0-1bd8-4812-83d2-f793587e9548",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: implenenting two helper functions\n",
    "Impelment these two functions:\n",
    "1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n",
    "2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n",
    "\n",
    "Note that the result of a Spark SQL query is itself a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5fce811-7dd2-4602-a243-18a36754c302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_query(query: \"a SQL query string\", df: \"the DataFrame that the query will be executed on\") -> Any:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    return spark.sql(query).collect()[0][0]\n",
    "\n",
    "def run_query2(query: \"a SQL query string\", df1: \"DataFrame A\", df2: \"DataFrame B\") -> Any:\n",
    "    df1.createOrReplaceTempView(\"df1\")\n",
    "    df2.createOrReplaceTempView(\"df2\")\n",
    "    return spark.sql(query).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a021cce-880b-42b1-a09d-6d2c2222124e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7de10aba-7e77-4baa-af35-2b4e37916071",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: writing a few queries\n",
    "Write the following queries in SQL to be executed by Spark in the next cell.\n",
    "\n",
    "1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n",
    "2. 'q2': find the number users\n",
    "3. 'q3': find the 'Id' of the user who posted most number of answers\n",
    "4. 'q4': find the number of questions\n",
    "5. 'q5': find the display name of the user who posted most number of comments\n",
    "\n",
    "Note that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3273ef7-7f4d-4b5d-bcf7-9935c952e26d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q1 = \"SELECT * FROM df ORDER BY CreationDate DESC limit 1\"\n",
    "\n",
    "## YOUR IMPLEMENTATION ##\n",
    "q2 = \"SELECT COUNT(*) FROM df\"\n",
    "\n",
    "## YOUR IMPLEMENTATION ##\n",
    "q3 = \"SELECT OwnerUserId, COUNT(*) AS antall FROM df GROUP BY OwnerUserId ORDER BY antall DESC\"\n",
    "\n",
    "## YOUR IMPLEMENTATION ##\n",
    "q4 = \"SELECT COUNT(*) FROM df WHERE PostTypeId=1\"\n",
    "\n",
    "## YOUR IMPLEMENTATION ##\n",
    "q5 = \"SELECT DisplayName, COUNT(*) AS antall FROM df2 JOIN df1 ON df2.UserId = df1.Id GROUP BY DisplayName ORDER BY antall DESC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46400bac-c11c-4bee-81a8-67d42e3ca968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementations by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ca3fb9-427c-425d-b334-0df9c208a21b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/unittest.status+json": {
         "color": "yellow",
         "message": "",
         "previous": 0
        },
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "37ad176b-03cea878c0fe216cd5db80d2"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".....\n----------------------------------------------------------------------\nRan 5 tests in 6.694s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/unittest.status+json": {
         "color": "lightgreen",
         "message": ".....\n----------------------------------------------------------------------\nRan 5 tests in 6.694s\n\nOK\n",
         "previous": 0
        },
        "text/plain": "Success"
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "37ad176b-03cea878c0fe216cd5db80d2"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       ".....\n",
       "----------------------------------------------------------------------\n",
       "Ran 5 tests in 6.694s\n",
       "\n",
       "OK\n",
       "Out[62]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": ".....\n----------------------------------------------------------------------\nRan 5 tests in 6.694s\n\nOK\nOut[62]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "class TestTask2(unittest.TestCase):\n",
    "    def test_q1(self):\n",
    "        # find the id of the most recent post\n",
    "        r = run_query(q1, posts_df)\n",
    "        self.assertEqual(r, 95045)\n",
    "\n",
    "    def test_q2(self):\n",
    "        # find the number of the users\n",
    "        r = run_query(q2, users_df)\n",
    "        self.assertEqual(r, 91616)\n",
    "\n",
    "    def test_q3(self):\n",
    "        # find the user id of the user who posted most number of answers\n",
    "        r = run_query(q3, posts_df)\n",
    "        self.assertEqual(r, 64377)\n",
    "\n",
    "    def test_q4(self):\n",
    "        # find the number of questions\n",
    "        r = run_query(q4, posts_df)\n",
    "        self.assertEqual(r, 28950)\n",
    "\n",
    "    def test_q5(self):\n",
    "        # find the display name of the user who posted most number of comments\n",
    "        r = run_query2(q5, users_df, comments_df)\n",
    "        self.assertEqual(r, \"Neil Slater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8a2e0c-7e72-410b-b385-00503c0bc136",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n",
    "2. When do you suggest using RDDs instead of using DataFrames?\n",
    "3. What is the main benefit of using DataSets instead of DataFrames?\n",
    "\n",
    "Write your answers in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c33ca21-1672-4c0c-a876-f73cbb8f65b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. The principal data structure of the Spark system is an acronym called RDD, which stands for Resilient Distributed Dataset. This data structure relies on transformations and actions at a low level, as previously explained in Task1. Additionally, Spark's DataFrame is a distributed data structure that is built on top of RDDs. DataFrames utilize a columnar schema, resembling a table in a relational database with named columns. However, it's essential to note that DataFrames cannot capture unstructured data, such as data streams. On the other hand, Datasets build upon DataFrames and provide an object-oriented interface that can be used for both structured and unstructured data. Both DataFrames and Datasets can automatically detect a schema, unlike RDDs, where the schema must be explicitly defined.\n",
    "\n",
    "\n",
    "2. Compared to DataFrames, RDDs offer greater flexibility and can serve as a general-purpose data structure in Spark. Thus, if your data does not conform to the tabular structure of DataFrames, RDDs could be a more suitable option.\n",
    "\n",
    "\n",
    "3. One of the main benefits DataSets can provide type-safety. This can help catch errors earlier rather than have them suddenly appear when you run the program. It has about the same perfomance as well."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Task2",
   "notebookOrigID": 2725054997030500,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
