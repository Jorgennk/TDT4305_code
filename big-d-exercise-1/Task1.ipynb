{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85d8965a-c0b4-4802-abb8-05fc5e21ce59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37c623c5-86c0-48ee-85de-5d514b4b334f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Joachim Maksim\n",
    "\n",
    "JÃ¸rgen Katralen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "324eb902-51f2-44b0-8ff1-730efac9900c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Out[27]: True"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Out[27]: True",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deleting tables left from previous runs in case they still exist after deleting an inactive cluster\n",
    "dbutils.fs.rm(\"/user\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60b530d6-b580-4de2-affb-aad878f4da86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\r\n",
       "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-31e318ea-344d-49a0-be2c-8e2850ca45e8/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-31e318ea-344d-49a0-be2c-8e2850ca45e8/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n",
    "!pip install -q ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023f4d10-e729-450c-8d1a-50d494b488d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading PySpark modules that we need\n",
    "import unittest\n",
    "from collections import Counter\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcda919c-b51e-4b61-9d61-80cc24f2d15e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: defining the schema for the data\n",
    "Typically, the first thing to do before loading the data into a Spark cluster is to define the schema for the data. Look at the schema for 'badges' and try to define the schema for other tables similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab8bdba5-f6d6-43bf-bed9-763d99cfcc91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining a schema for 'badges' table\n",
    "badges_schema = StructType([StructField('UserId', IntegerType(), False),\n",
    "                            StructField('Name', StringType(), False),\n",
    "                            StructField('Date', TimestampType(), False),\n",
    "                            StructField('Class', IntegerType(), False)])\n",
    "\n",
    "# Defining a schema for 'posts' table\n",
    "posts_schema = StructType([StructField('Id', IntegerType(), False),\n",
    "                            StructField('ParentId', IntegerType(), True),\n",
    "                            StructField('PostTypeId', IntegerType(), False),\n",
    "                            StructField('CreationDate', TimestampType(), False),\n",
    "                            StructField('Score', IntegerType(), False),\n",
    "                            StructField('ViewCount', IntegerType(), False),\n",
    "                            StructField('Body', StringType(), False),\n",
    "                            StructField('OwnerUserId', IntegerType(), False),\n",
    "                            StructField('LastActivityDate', TimestampType(), False),\n",
    "                            StructField('Title', StringType(), True),\n",
    "                            StructField('Tags', StringType(), True),\n",
    "                            StructField('AnswerCount', IntegerType(), False),\n",
    "                            StructField('CommentCount', IntegerType(), False),\n",
    "                            StructField('FavoriteCount', IntegerType(), False),\n",
    "                            StructField('ClosedDate', TimestampType(), True)])\n",
    "## YOUR IMPLEMENTATION ##\n",
    "\n",
    "# Defining a schema for 'users' table\n",
    "users_schema = StructType([StructField('Id', IntegerType(), False),\n",
    "                            StructField('Reputation', IntegerType(), False),\n",
    "                            StructField('CreationDate', TimestampType(), False),\n",
    "                            StructField('DisplayName', StringType(), False),\n",
    "                            StructField('LastAccessDate', TimestampType(), False),\n",
    "                            StructField('AboutMe', StringType(), True),\n",
    "                            StructField('Views', IntegerType(), False),\n",
    "                            StructField('UpVotes', IntegerType(), False),\n",
    "                            StructField('DownVotes', IntegerType(), False)])\n",
    "## YOUR IMPLEMENTATION ##\n",
    "\n",
    "# Defining a schema for 'comments' table\n",
    "comments_schema = StructType([StructField('PostId', IntegerType(), False),\n",
    "                            StructField('Score', IntegerType(), False),\n",
    "                            StructField('Text', StringType(), False),\n",
    "                            StructField('CreationDate', TimestampType(), False),\n",
    "                            StructField('UserId', IntegerType(), False)])\n",
    "## YOUR IMPLEMENTATION ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab2cfe2-0961-4a22-8fb1-9a6f9191fbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: implementing two helper functions\n",
    "Next, we need to implement two helper functions:\n",
    "1. 'load_csv' that as input argument receives path for a CSV file and a schema and loads the CSV pointed by the path into a Spark DataFrame and returns the DataFrame;\n",
    "2. 'save_df' receives a Spark DataFrame and saves it as a Parquet file on DBFS.\n",
    "\n",
    "Note that the column separator in CSV files is TAB character ('\\t') and the first row includes the name of the columns. \n",
    "\n",
    "BTW, DBFS is the name of the distributed filesystem used by Databricks Community Edition to store and access data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "089f87ff-f2c8-4ac8-8449-cec251c502f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_csv(source_file: \"path for the CSV file to load\", schema: \"schema for the CSV file being loaded as a DataFrame\") -> DataFrame:\n",
    "    return spark.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", True).schema(schema).load(source_file)\n",
    "\n",
    "def save_df(df: \"DataFrame to be saved\", table_name: \"name under which the DataFrame will be saved\") -> None:\n",
    "    df.write.format(\"parquet\").save(f\"/user/hive/warehouse/{table_name}\") # path was not specified, discovered in task 2\n",
    "    ## YOUR IMPLEMENTATION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39bc683c-b37a-4842-8bf8-004620b17cca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8903e903-7e4f-4c15-99bd-c9129a601fde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementation by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully. Moreover, at the end there should be four Parquet files named 'badges', 'comments', 'posts', and 'users' in '/user/hive/warehouse'.\n",
    "\n",
    "Note that we assumed that the data for the project has already been stored on DBFS on the '/FileStore/tables/' path. (I mean as 'badges_csv.gz', 'comments_csv.gz', 'posts_csv.gz', and 'users_csv.gz'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd470d59-2571-4b7d-b022-9ee8f7c3e281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/unittest.status+json": {
         "color": "yellow",
         "message": "",
         "previous": 0
        },
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "183035ab-894ac9fa07e74bb2c97e8fb5"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".....\n----------------------------------------------------------------------\nRan 5 tests in 22.090s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/unittest.status+json": {
         "color": "lightgreen",
         "message": ".....\n----------------------------------------------------------------------\nRan 5 tests in 22.090s\n\nOK\n",
         "previous": 0
        },
        "text/plain": "Success"
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "183035ab-894ac9fa07e74bb2c97e8fb5"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       ".....\n",
       "----------------------------------------------------------------------\n",
       "Ran 5 tests in 22.090s\n",
       "\n",
       "OK\n",
       "Out[29]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": ".....\n----------------------------------------------------------------------\nRan 5 tests in 22.090s\n\nOK\nOut[29]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>",
       "datasetInfos": [],
       "metadata": {},
       "name": null,
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "class TestTask1(unittest.TestCase):\n",
    "   \n",
    "    # test 1\n",
    "    def test_load_badges(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/badges_csv.gz\", schema=badges_schema)\n",
    "        self.assertIsNotNone(result, \"Badges dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 105640, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower, ['UserId', 'Name', 'Date', 'Class']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 2\n",
    "    def test_load_posts(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/posts_csv.gz\", schema=posts_schema)\n",
    "        self.assertIsNotNone(result, \"Posts dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 61432, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower,\n",
    "                                   ['Id', 'ParentId', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId',\n",
    "                                    'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n",
    "                                    'ClosedDate']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 3\n",
    "    def test_load_comments(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/comments_csv.gz\", schema=comments_schema)\n",
    "        self.assertIsNotNone(result, \"Comments dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 58735, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower, ['PostId', 'Score', 'Text', 'CreationDate', 'UserId']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 4\n",
    "    def test_load_users(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/users_csv.gz\", schema=users_schema)\n",
    "        self.assertIsNotNone(result, \"Users dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 91616, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower,\n",
    "                                   ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'AboutMe',\n",
    "                                    'Views', 'UpVotes', 'DownVotes']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    # test 5\n",
    "    def test_save_dfs(self):\n",
    "        \"\"\" This test doesn't actually assert anything, so it only fails if the code fails to execute... \"\"\"\n",
    "        dfs = [(\"/FileStore/tables/users_csv.gz\", users_schema, \"users\"),\n",
    "               (\"/FileStore/tables/badges_csv.gz\", badges_schema, \"badges\"),\n",
    "               (\"/FileStore/tables/comments_csv.gz\", comments_schema, \"comments\"),\n",
    "               (\"/FileStore/tables/posts_csv.gz\", posts_schema, \"posts\")\n",
    "               ]\n",
    "\n",
    "        for i in dfs:\n",
    "            df = load_csv(source_file=i[0], schema=i[1])\n",
    "            save_df(df, i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f99b257-8618-4796-aeb0-d9446863c259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What do the terms 'Spark Application', 'SparkSession', 'Transformations', 'Action', and 'Lazy Evaluation' mean in the context of Spark?\n",
    "\n",
    "Write your descriptions in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91ec9fda-7848-4f01-8a36-3b82b78be007",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Spark Application\n",
    "The spark application is the application that we run to generate results through our code. It takes care of the Driver and Worker processes. These worker processes execute the code we write and takes care of the computations, the driver keeps track of and organizes them. \n",
    "\n",
    "##### SparkSession\n",
    "A  spark session is the entry point of a Spark application. You can look at it like an interface for the Spark application. It wraps up a lot of functionality (like hive, SQL and Spark) for ease of use.\n",
    "\n",
    "##### Transformations\n",
    "Transformations (does as the name suggest) transform/changes the data you have through the code. It's a function that produces new RDD from existing RDDs. Each time it creates new RDD we apply a transformation. An RDD is one of the fundamental data strructures we can use in Spark. They are collections of objects that are fault-tolerant \n",
    "\n",
    "\n",
    "##### Action\n",
    "Actions in Spark RDD is the operations that return raw values. Everything that doesn't return an RDD is considered an action in Spark.\n",
    "\n",
    "##### Lazy Evaluation\n",
    "This is a concept in Spark that makes it so you can apply an indefinite number of transformations to the data, but Spark won't execute them before an action is called. Thus it can save some time or make computations more efficient by freeing up processing power until it is needed."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Task1",
   "notebookOrigID": 2725054997030485,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
